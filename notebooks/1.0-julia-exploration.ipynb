{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b06d9ac-fa77-4265-890d-086c4d0c5a3e",
   "metadata": {},
   "source": [
    "# Project: Road Accidents in France (2005â€“2023)\n",
    "Based on the Annual Road Traffic Accident Injury Database (BAAC)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project aims to **predict the severity of road accidents in France** using historical data collected between 2005 and 2023. The data is sourced from the BAAC (Bulletin dâ€™Analyse des Accidents Corporels de la Circulation) and includes detailed records of injury accidents.\n",
    "\n",
    "The main objectives include:\n",
    "\n",
    "1. Studying and cleaning the dataset.\n",
    "2. Extracting relevant characteristics for severity prediction.\n",
    "3. Developing a model to evaluate accident severity.\n",
    "4. Scoring risk zones using:\n",
    "   - Meteorological information\n",
    "   - Geographic data\n",
    "\n",
    "Once trained, the model will be validated against historical data.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Description\n",
    "\n",
    "Each accident involving injuriesâ€”occurring on public roads, involving at least one vehicle, and resulting in at least one medically treated victimâ€”is recorded by a law enforcement unit (e.g., police, gendarmerie). This information is compiled into the *Injury Accident Analysis Bulletin*. These bulletins form the national **BAAC File**, administered by the **National Interministerial Road Safety Observatory (ONISR)**.\n",
    "\n",
    "The datasets cover all injury-related road accidents across:\n",
    "\n",
    "- **Mainland France**\n",
    "- **Overseas Departments**: Guadeloupe, French Guiana, Martinique, RÃ©union, and Mayotte (since 2012)\n",
    "- **Overseas Territories**: Saint-Pierre and Miquelon, Saint-BarthÃ©lemy, Saint-Martin, Wallis and Futuna, French Polynesia, New Caledonia (since 2019)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Source and Storage\n",
    "\n",
    "Data was downloaded from:  \n",
    "ðŸ”— [data.gouv.fr â€“ BAAC Datasets](https://www.data.gouv.fr/en/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2023/)  \n",
    "Saved under: `data/raw/`\n",
    "\n",
    "The annual data files include:\n",
    "\n",
    "- `CaractÃ©ristiques` â€“ Accident characteristics\n",
    "- `Lieux` â€“ Location data\n",
    "- `VÃ©hicules` â€“ Vehicle details\n",
    "- `Usagers` â€“ User information\n",
    "\n",
    "An additional dataset, `vehicules-immatricules-baac` (registered vehicles, 2009â€“2022), is available but **not used currently** due to incomplete yearly coverage, which could hinder machine learning performance.\n",
    "\n",
    "All filenames have been standardized to the format:  \n",
    "`category_year.csv` (e.g., `Caracteristiques_2020.csv`)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Integration\n",
    "\n",
    "To streamline processing and analysis, all files are joined using the `num_acc` (accident ID) column, forming a unified dataset for each accident. \n",
    "\n",
    "---\n",
    "\n",
    "## Column Reference\n",
    "\n",
    "The following table provides a detailed description of key columns across the datasets. Each variable is encoded; refer to the accompanying documentation for complete details:\n",
    "\n",
    "- `description-des-bases-de-donnees-annuelles.pdf` â€“ (in French)\n",
    "- `Caracteristics.docx` â€“ (English translation of column meanings)  \n",
    "Both documents are stored in the `references/` folder.\n",
    "\n",
    "Short description of the columns \n",
    "| Column                | Description |\n",
    "|-----------------------|-----------------------------------|\n",
    "| **num_acc**           | Unique identifier of the accident, assigned by the law enforcement. Join key between the files characteristics, locations, vehicles and users. |\n",
    "| **an**                | Year of the accident. |\n",
    "| **mois**              | Month of the accident. |\n",
    "| **jour**              | Day of the accident. |\n",
    "| **hrmn**              | Hour and minutes of the accident (format hhmm). |\n",
    "| **lum**               | Lighting conditions at the time of the accident: 1 = Full daylight, 2 = Twilight or dawn, 3 = Night without public lighting, 4 = Night with public lighting not lit, 5 = Night with public lighting lit. |\n",
    "| **agg**               | Urban area indicator: 1 = Outside urban area, 2 = Inside urban area. |\n",
    "| **int**               | Type of intersection: 1 = Outside intersection, 2 = Intersection in X, 3 = Intersection in T, 4 = Intersection in Y, 5 = More than 4 branches, 6 = Roundabout, 7 = Square, 8 = Level crossing, 9 = Other intersection. |\n",
    "| **atm**               | Atmospheric conditions at the time of the accident: -1 = Not specified, 1 = Normal, 2 = Light rain, 3 = Heavy rain, 4 = Snowâ€“hail, 5 = Fogâ€“smoke, 6 = Strong windâ€“storm, 7 = Dazzling, 8 = Overcast, 9 = Other. |\n",
    "| **col**               | Type of collision: -1 = Not specified, 1 = Two vehicles â€“ head-on, 2 = Two vehicles â€“ rear-end, 3 = Two vehicles â€“ side, 4 = Three vehicles or more â€“ in chain, 5 = Three vehicles or more â€“ multiple collisions, 6 = Other collision, 7 = Without collision. |\n",
    "| **com**               | INSEE code of the municipality of the accident. |\n",
    "| **adr**               | Postal address of the accident, when the accident is located inside urban area. |\n",
    "| **gps**               | GPS coordinates (raw text). |\n",
    "| **lat**               | Latitude (decimal degrees). |\n",
    "| **long**              | Longitude (decimal degrees). |\n",
    "| **dep**               | INSEE department code of the accident. |\n",
    "| **catr**              | Road category: 1 = Highway, 2 = National road, 3 = Departmental road, 4 = Communal road, 5 = Outside public network, 6 = Parking lot, 7 = Urban metropolitan road, 9 = Other. |\n",
    "| **voie**              | Name or number of the road at the place of the accident. |\n",
    "| **v1**                | Numeric sub-identifier for the road (e.g., \"2 bis\" expressed as number). |\n",
    "| **v2**                | Alphanumeric sub-identifier for the road. |\n",
    "| **circ**              | Traffic regime: -1 = Not specified, 1 = One way, 2 = Two way, 3 = Separated carriageways, 4 = Variable. |\n",
    "| **nbv**               | Number of lanes for vehicles at the section of the road where the accident happened. |\n",
    "| **pr**                | PR number (reference point, upstream terminal) â€” -1 if not filled. |\n",
    "| **pr1**               | Distance to PR in meters (relative to the upstream terminal) â€” -1 if not filled. |\n",
    "| **vosp**              | Reserved lane: -1 = Not specified, 0 = Not applicable, 1 = Cycle track, 2 = Cycle lane, 3 = Reserved lane. |\n",
    "| **prof**              | Longitudinal profile of the road: -1 = Not specified, 1 = Flat, 2 = Slope, 3 = Top of hill, 4 = Bottom of hill. |\n",
    "| **plan**              | Road plan: -1 = Not specified, 1 = Straight, 2 = Left curve, 3 = Right curve, 4 = \"S\" curve. |\n",
    "| **lartpc**            | Width of the central reservation (in meters). |\n",
    "| **larrout**           | Width of the carriageway assigned to vehicles (in meters). |\n",
    "| **surf**              | Surface condition: -1 = Not specified, 1 = Normal, 2 = Wet, 3 = Puddles, 4 = Flooded, 5 = Snow, 6 = Mud, 7 = Icy, 8 = Fatty/oily, 9 = Other. |\n",
    "| **infra**             | Infrastructure: -1 = Not specified, 0 = None, 1 = Tunnel, 2 = Bridge, 3 = Interchange or ramp, 4 = Railway, 5 = Crossing, 6 = Pedestrian area, 7 = Toll zone, 8 = Construction, 9 = Other. |\n",
    "| **situ**              | Situation: -1 = Not specified, 0 = None, 1 = On roadway, 2 = On emergency lane, 3 = On shoulder, 4 = On sidewalk, 5 = On cycle path, 6 = On special route, 8 = Other. |\n",
    "| **env1**              | Environment at the site of the accident (additional codings, see documentation if available). |\n",
    "| **senc**              | Direction of movement: -1 = Not specified, 0 = Unknown, 1 = Ascending, 2 = Descending, 3 = No reference. |\n",
    "| **catv**              | Category of vehicle involved: 01 = Bicycle, 07 = Passenger car, 13 = Heavy truck, 31 = Motorcycle, 37 = Bus, etc. (see full coding in documentation). |\n",
    "| **occutc**            | Number of people present in the vehicle (for public transport vehicles). |\n",
    "| **obs**               | Fixed obstacle hit: -1 = Not specified, 0 = Not applicable, 1 = Parked vehicle, 2 = Tree, 3 = Post, 4 = Rail guard, 5 = Concrete wall, 6 = Building, 7 = Fire hydrant, 8 = Lamp post, 9 = Other. |\n",
    "| **obsm**              | Mobile obstacle hit: -1 = Not specified, 0 = None, 1 = Pedestrian, 2 = Vehicle, 3 = Animal, 4 = Other. |\n",
    "| **choc**              | Initial point of impact: -1 = Not specified, 0 = None, 1 = Front, 2 = Front right, 3 = Front left, 4 = Rear, 5 = Rear right, 6 = Rear left, 7 = Side right, 8 = Side left. |\n",
    "| **manv**              | Main maneuver before the accident: -1 = Not specified, 1 = No change, 2 = Stopped, 3 = Stationary, 4 = Reversing, 5 = Parking, 6 = Starting, 7 = Overtaking right, 8 = Overtaking left, 9 = Changing lanes, 10 = U-turn, 11 = Turning right, 12 = Turning left, 13 = Other. |\n",
    "| **num_veh**           | Vehicle identifier in the accident, allows linking with occupants and users. Alphanumeric code. |\n",
    "| **place**             | Position occupied by the user in the vehicle or accident: 10 = Pedestrian (see documentation for other positions/seats). |\n",
    "| **catu**              | Category of user: 1 = Driver, 2 = Passenger, 3 = Pedestrian. |\n",
    "| **grav**              | Severity of injury for the user: 1 = Unharmed, 2 = Killed, 3 = Hospitalized, 4 = Light injury. |\n",
    "| **sexe**              | Gender of the user: 1 = Male, 2 = Female. |\n",
    "| **trajet**            | Reason for the journey: -1 or 0 = Not specified, 1 = Home-work, 2 = Home-school, 3 = Shopping, 4 = Professional, 5 = Leisure, 9 = Other. |\n",
    "| **secu**              | Safety equipment used by the user. Up to 2018: 0 = None, 1 = Seat belt, 2 = Helmet, 3 = Child seat, 4 = Reflective vest, 5 = Other (see secu1/secu2/secu3 for 2019+). |\n",
    "| **locp**              | Location of the pedestrian: -1 = Not specified, 1 = On roadway, 2 = On shoulder, 3 = On crosswalk, 4 = On cycle path, 5 = On sidewalk, 6 = On special route, 7 = Other. |\n",
    "| **actp**              | Action of the pedestrian: -1 = Not specified, 0 = Not applicable, 1 = Heading toward the vehicle, 2 = Moving away, 3 = Crossing, 4 = Waiting, 5 = Playing, 6 = Working, 9 = Other. |\n",
    "| **etatp**             | Pedestrian's situation: -1 = Not specified, 1 = Alone, 2 = Accompanied, 3 = In group. |\n",
    "| **an_nais**           | Year of birth of the user involved in the accident. |\n",
    "| **num_veh_usag**      | Vehicle identifier (user table), alphanumeric code. |\n",
    "| **vma**               | Maximum speed limit at the location and time of the accident (in km/h). |\n",
    "| **id_vehicule**       | Unique identifier of the vehicle (in users and vehicles files). |\n",
    "| **motor**             | Engine type: -1 = Not specified, 1 = Internal combustion, 2 = Hybrid, 3 = Electric, 4 = Hydrogen, 5 = Other. |\n",
    "| **id_vehicule_usag**  | Unique vehicle identifier from the users file. |\n",
    "| **secu1**             | 1st safety equipment used (from 2019 onwards). |\n",
    "| **secu2**             | 2nd safety equipment used (from 2019 onwards). |\n",
    "| **secu3**             | 3rd safety equipment used (from 2019 onwards). |\n",
    "| **id_usager**         | Unique identifier of the user (in the users file). |\n",
    "\n",
    "---\n",
    "\n",
    "## Important Considerations\n",
    "\n",
    "- **Runaway Users:**  \n",
    "  Since 2021, data on users who fled the scene has been included.  \n",
    "  This results in missing information such as sex, age, and injury severity (unharmed, slightly injured, hospitalized).\n",
    "\n",
    "- **Missing Data:**  \n",
    "  Most variables across the four main files may contain:\n",
    "  - Empty cells  \n",
    "  - Zeros  \n",
    "  - Periods (`.`)  \n",
    "  These indicate either that law enforcement did not provide the data or that the information was not considered relevant.  \n",
    "  Some categories also use `-1` to denote 'not specified'.\n",
    "\n",
    "- **Hospitalized Injured Persons:**  \n",
    "  Since 2018, the classification of hospitalized injured persons has changed due to updates in the law enforcement data entry process.  \n",
    "  As a result, this indicator is not comparable with earlier years and has not been certified by the Public Statistics Authority since 2019.\n",
    "\n",
    "---\n",
    "\n",
    "## This Notebook Overview\n",
    "\n",
    "This notebook outlines the key steps involved in preparing the BAAC dataset for modeling accident severity:\n",
    "\n",
    "1. **Merge all files**  \n",
    "   Combine the four main datasets (`CaractÃ©ristiques`, `Lieux`, `VÃ©hicules`, `Usagers`) into a single table using the `num_acc` field as the unique accident identifier.\n",
    "\n",
    "2. **Clean and normalize**  \n",
    "   Handle missing or inconsistent data across fields. This includes:\n",
    "   - Replacing placeholder values (`-1`, `.`, `0`) with proper missing value indicators\n",
    "   - Standardizing data types (e.g., dates, coordinates)\n",
    "   - Removing or flagging incomplete records\n",
    "\n",
    "3. **Feature engineering**  \n",
    "   Extract or derive new variables that may improve model accuracy, such as:\n",
    "   - Aggregated weather and lighting conditions\n",
    "   - Geographical zones or clusters based on GPS\n",
    "   - Temporal variables like time of day, weekday/weekend, season\n",
    "   - User demographics (age groups, sex, role)\n",
    "\n",
    "Further steps will include model training, evaluation, and visualization of severity predictions and high-risk zones.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545f8a-ac64-498a-b23a-847278974c82",
   "metadata": {},
   "source": [
    "### Merge all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1f7cff-f145-4aa4-855d-34e822b1954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the packages \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e40a180-9c84-4b6b-829a-d746acc83139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as: ../data/processed\\accidents_merged_2005_2023.csv\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"../data/raw\"\n",
    "processed_path = \"../data/processed\"\n",
    "output_file = os.path.join(processed_path, \"accidents_merged_2005_2023.csv\")\n",
    "\n",
    "#Some files have different delimiter and a typical French encoder ISO-8859-1\n",
    "def robust_read_csv(fpath, encoding=\"ISO-8859-1\"): \n",
    "    for delim in [\",\", \";\", \"\\t\"]:\n",
    "        df = pd.read_csv(fpath, delimiter=delim, encoding=encoding, on_bad_lines=\"skip\", low_memory=False)\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace('\"', '')\n",
    "        # Rename for accident_id\n",
    "        if 'accident_id' in df.columns:\n",
    "            df = df.rename(columns={'accident_id': 'num_acc'}) #some datasets had different name for the num_acc key column\n",
    "        if 'num_acc' in df.columns:\n",
    "            return df\n",
    "    print(f\"!! Could not properly split columns for {fpath}, got: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "all_years = []\n",
    "for year in range(2005, 2024):\n",
    "    files = {\n",
    "        \"carac\": os.path.join(raw_path, f\"caracteristiques_{year}.csv\"),\n",
    "        \"lieux\": os.path.join(raw_path, f\"lieux_{year}.csv\"),\n",
    "        \"vehic\": os.path.join(raw_path, f\"vehicules_{year}.csv\"),\n",
    "        \"usag\": os.path.join(raw_path, f\"usagers_{year}.csv\"),\n",
    "    }\n",
    "    dfs = {k: robust_read_csv(fpath) for k, fpath in files.items()}\n",
    "\n",
    "    if all('num_acc' in df.columns for df in dfs.values()):\n",
    "        df = dfs[\"carac\"].merge(dfs[\"lieux\"], on=\"num_acc\", how=\"inner\", suffixes=('', '_lieux')) #Keep only rows that have a match in both DataFrames\n",
    "        df = df.merge(dfs[\"vehic\"], on=\"num_acc\", how=\"inner\", suffixes=('', '_vehic'))\n",
    "        df = df.merge(dfs[\"usag\"], on=\"num_acc\", how=\"inner\", suffixes=('', '_usag'))\n",
    "        df[\"an\"] = year  # Replace 'an' column with the current year for unification, for some years it was 5 instead of 2005\n",
    "        all_years.append(df)\n",
    "    else:\n",
    "        print(f\"Skipping year {year}: 'num_acc' not found in all files.\")\n",
    "\n",
    "# Combine all years and save\n",
    "merged_df = pd.concat(all_years, ignore_index=True)\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "print(f\"Merged file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70963b-7144-4bba-b529-19bb4402ff44",
   "metadata": {},
   "source": [
    "### Quick data health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446ee84a-8ae5-428e-bf3c-2d2933d06847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>atm</th>\n",
       "      <th>col</th>\n",
       "      <th>...</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>num_veh_usag</th>\n",
       "      <th>vma</th>\n",
       "      <th>id_vehicule</th>\n",
       "      <th>motor</th>\n",
       "      <th>id_vehicule_usag</th>\n",
       "      <th>secu1</th>\n",
       "      <th>secu2</th>\n",
       "      <th>secu3</th>\n",
       "      <th>id_usager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acc    an  mois  jour  hrmn  lum  agg  int  atm  col  ... an_nais  \\\n",
       "0  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1976.0   \n",
       "1  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1968.0   \n",
       "2  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1964.0   \n",
       "3  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  2004.0   \n",
       "4  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1998.0   \n",
       "5  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1991.0   \n",
       "6  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1976.0   \n",
       "7  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1968.0   \n",
       "8  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1964.0   \n",
       "9  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  2004.0   \n",
       "\n",
       "  num_veh_usag vma id_vehicule motor id_vehicule_usag  secu1 secu2  secu3  \\\n",
       "0          A01 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "1          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "2          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "3          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "4          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "5          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "6          A01 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "7          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "8          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "9          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "\n",
       "  id_usager  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "5       NaN  \n",
       "6       NaN  \n",
       "7       NaN  \n",
       "8       NaN  \n",
       "9       NaN  \n",
       "\n",
       "[10 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_df.head(10))\n",
    "#For the better understanding, I want to look at all columns, so I extract 100 random rows as a separate file\n",
    "#df = pd.read_csv(\"../data/processed/accidents_merged_2005_2023.csv\", low_memory=False)\n",
    "#df_sample = df.sample(n=100, random_state=42)\n",
    "#df_sample.to_csv(\"../data/processed/accidents_sample_100_random.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc51cdc-5fb2-4b44-9975-340b0cb02490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5314184 entries, 0 to 5314183\n",
      "Data columns (total 60 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   num_acc           int64  \n",
      " 1   an                int64  \n",
      " 2   mois              int64  \n",
      " 3   jour              int64  \n",
      " 4   hrmn              object \n",
      " 5   lum               int64  \n",
      " 6   agg               int64  \n",
      " 7   int               int64  \n",
      " 8   atm               float64\n",
      " 9   col               float64\n",
      " 10  com               object \n",
      " 11  adr               object \n",
      " 12  gps               object \n",
      " 13  lat               object \n",
      " 14  long              object \n",
      " 15  dep               object \n",
      " 16  catr              float64\n",
      " 17  voie              object \n",
      " 18  v1                float64\n",
      " 19  v2                object \n",
      " 20  circ              float64\n",
      " 21  nbv               object \n",
      " 22  pr                object \n",
      " 23  pr1               object \n",
      " 24  vosp              float64\n",
      " 25  prof              float64\n",
      " 26  plan              float64\n",
      " 27  lartpc            object \n",
      " 28  larrout           object \n",
      " 29  surf              float64\n",
      " 30  infra             float64\n",
      " 31  situ              float64\n",
      " 32  env1              float64\n",
      " 33  senc              float64\n",
      " 34  catv              int64  \n",
      " 35  occutc            float64\n",
      " 36  obs               float64\n",
      " 37  obsm              float64\n",
      " 38  choc              float64\n",
      " 39  manv              float64\n",
      " 40  num_veh           object \n",
      " 41  place             float64\n",
      " 42  catu              int64  \n",
      " 43  grav              int64  \n",
      " 44  sexe              int64  \n",
      " 45  trajet            float64\n",
      " 46  secu              float64\n",
      " 47  locp              float64\n",
      " 48  actp              object \n",
      " 49  etatp             float64\n",
      " 50  an_nais           float64\n",
      " 51  num_veh_usag      object \n",
      " 52  vma               float64\n",
      " 53  id_vehicule       object \n",
      " 54  motor             float64\n",
      " 55  id_vehicule_usag  object \n",
      " 56  secu1             float64\n",
      " 57  secu2             float64\n",
      " 58  secu3             float64\n",
      " 59  id_usager         object \n",
      "dtypes: float64(29), int64(11), object(20)\n",
      "memory usage: 2.4+ GB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ac54d-7107-400e-bcbc-e4258cce6e3f",
   "metadata": {},
   "source": [
    "Unique value counts per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92838f11-7454-487c-b517-c212c724873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "2005    374561\n",
      "2006    362507\n",
      "2007    356228\n",
      "2008    322196\n",
      "2009    311706\n",
      "2023    309341\n",
      "2010    288112\n",
      "2011    281675\n",
      "2012    263194\n",
      "2017    260392\n",
      "2016    257286\n",
      "2019    253488\n",
      "2014    248642\n",
      "2018    248406\n",
      "2021    248187\n",
      "2015    245706\n",
      "2013    242163\n",
      "2022    241487\n",
      "2020    198907\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['an'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd82ffa9-21c0-4ed0-a652-fd4a243fed21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>atm</th>\n",
       "      <th>col</th>\n",
       "      <th>catr</th>\n",
       "      <th>...</th>\n",
       "      <th>trajet</th>\n",
       "      <th>secu</th>\n",
       "      <th>locp</th>\n",
       "      <th>etatp</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>vma</th>\n",
       "      <th>motor</th>\n",
       "      <th>secu1</th>\n",
       "      <th>secu2</th>\n",
       "      <th>secu3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.314184e+06</td>\n",
       "      <td>5.314184e+06</td>\n",
       "      <td>5.314184e+06</td>\n",
       "      <td>5.314184e+06</td>\n",
       "      <td>5.314184e+06</td>\n",
       "      <td>5.314184e+06</td>\n",
       "      <td>5.314184e+06</td>\n",
       "      <td>5.313921e+06</td>\n",
       "      <td>5.314089e+06</td>\n",
       "      <td>5.314182e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.313106e+06</td>\n",
       "      <td>4.002731e+06</td>\n",
       "      <td>5.194985e+06</td>\n",
       "      <td>5.194893e+06</td>\n",
       "      <td>5.292557e+06</td>\n",
       "      <td>1.251410e+06</td>\n",
       "      <td>1.251410e+06</td>\n",
       "      <td>1.251410e+06</td>\n",
       "      <td>1.251410e+06</td>\n",
       "      <td>1.251410e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.013328e+11</td>\n",
       "      <td>2.013328e+03</td>\n",
       "      <td>6.701324e+00</td>\n",
       "      <td>1.558823e+01</td>\n",
       "      <td>1.860729e+00</td>\n",
       "      <td>1.616256e+00</td>\n",
       "      <td>1.794296e+00</td>\n",
       "      <td>1.577097e+00</td>\n",
       "      <td>3.598634e+00</td>\n",
       "      <td>3.223352e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.090887e+00</td>\n",
       "      <td>1.690227e+01</td>\n",
       "      <td>1.232997e-02</td>\n",
       "      <td>-1.731289e-01</td>\n",
       "      <td>1.975504e+03</td>\n",
       "      <td>6.181612e+01</td>\n",
       "      <td>1.219787e+00</td>\n",
       "      <td>1.901712e+00</td>\n",
       "      <td>1.102819e+00</td>\n",
       "      <td>-9.200246e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.659027e+08</td>\n",
       "      <td>5.659080e+00</td>\n",
       "      <td>3.374824e+00</td>\n",
       "      <td>8.759571e+00</td>\n",
       "      <td>1.470012e+00</td>\n",
       "      <td>4.862969e-01</td>\n",
       "      <td>1.641354e+00</td>\n",
       "      <td>1.622932e+00</td>\n",
       "      <td>1.711043e+00</td>\n",
       "      <td>1.261386e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.700352e+00</td>\n",
       "      <td>1.704492e+01</td>\n",
       "      <td>7.889048e-01</td>\n",
       "      <td>5.367348e-01</td>\n",
       "      <td>1.855589e+01</td>\n",
       "      <td>2.697713e+01</td>\n",
       "      <td>1.038167e+00</td>\n",
       "      <td>2.239605e+00</td>\n",
       "      <td>3.092796e+00</td>\n",
       "      <td>8.557038e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.005000e+11</td>\n",
       "      <td>2.005000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.896000e+03</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.008001e+11</td>\n",
       "      <td>2.008000e+03</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.963000e+03</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.013000e+11</td>\n",
       "      <td>2.013000e+03</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.978000e+03</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.018000e+11</td>\n",
       "      <td>2.018000e+03</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.989000e+03</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.023001e+11</td>\n",
       "      <td>2.023000e+03</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.023000e+03</td>\n",
       "      <td>9.010000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            num_acc            an          mois          jour           lum  \\\n",
       "count  5.314184e+06  5.314184e+06  5.314184e+06  5.314184e+06  5.314184e+06   \n",
       "mean   2.013328e+11  2.013328e+03  6.701324e+00  1.558823e+01  1.860729e+00   \n",
       "std    5.659027e+08  5.659080e+00  3.374824e+00  8.759571e+00  1.470012e+00   \n",
       "min    2.005000e+11  2.005000e+03  1.000000e+00  1.000000e+00 -1.000000e+00   \n",
       "25%    2.008001e+11  2.008000e+03  4.000000e+00  8.000000e+00  1.000000e+00   \n",
       "50%    2.013000e+11  2.013000e+03  7.000000e+00  1.600000e+01  1.000000e+00   \n",
       "75%    2.018000e+11  2.018000e+03  1.000000e+01  2.300000e+01  2.000000e+00   \n",
       "max    2.023001e+11  2.023000e+03  1.200000e+01  3.100000e+01  5.000000e+00   \n",
       "\n",
       "                agg           int           atm           col          catr  \\\n",
       "count  5.314184e+06  5.314184e+06  5.313921e+06  5.314089e+06  5.314182e+06   \n",
       "mean   1.616256e+00  1.794296e+00  1.577097e+00  3.598634e+00  3.223352e+00   \n",
       "std    4.862969e-01  1.641354e+00  1.622932e+00  1.711043e+00  1.261386e+00   \n",
       "min    1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00  1.000000e+00   \n",
       "25%    1.000000e+00  1.000000e+00  1.000000e+00  2.000000e+00  3.000000e+00   \n",
       "50%    2.000000e+00  1.000000e+00  1.000000e+00  3.000000e+00  3.000000e+00   \n",
       "75%    2.000000e+00  2.000000e+00  1.000000e+00  5.000000e+00  4.000000e+00   \n",
       "max    2.000000e+00  9.000000e+00  9.000000e+00  7.000000e+00  9.000000e+00   \n",
       "\n",
       "       ...        trajet          secu          locp         etatp  \\\n",
       "count  ...  5.313106e+06  4.002731e+06  5.194985e+06  5.194893e+06   \n",
       "mean   ...  3.090887e+00  1.690227e+01  1.232997e-02 -1.731289e-01   \n",
       "std    ...  2.700352e+00  1.704492e+01  7.889048e-01  5.367348e-01   \n",
       "min    ... -1.000000e+00  0.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%    ...  0.000000e+00  1.100000e+01  0.000000e+00  0.000000e+00   \n",
       "50%    ...  4.000000e+00  1.100000e+01  0.000000e+00  0.000000e+00   \n",
       "75%    ...  5.000000e+00  2.100000e+01  0.000000e+00  0.000000e+00   \n",
       "max    ...  9.000000e+00  9.300000e+01  9.000000e+00  3.000000e+00   \n",
       "\n",
       "            an_nais           vma         motor         secu1         secu2  \\\n",
       "count  5.292557e+06  1.251410e+06  1.251410e+06  1.251410e+06  1.251410e+06   \n",
       "mean   1.975504e+03  6.181612e+01  1.219787e+00  1.901712e+00  1.102819e+00   \n",
       "std    1.855589e+01  2.697713e+01  1.038167e+00  2.239605e+00  3.092796e+00   \n",
       "min    1.896000e+03 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%    1.963000e+03  5.000000e+01  1.000000e+00  1.000000e+00 -1.000000e+00   \n",
       "50%    1.978000e+03  5.000000e+01  1.000000e+00  1.000000e+00  0.000000e+00   \n",
       "75%    1.989000e+03  8.000000e+01  1.000000e+00  2.000000e+00  0.000000e+00   \n",
       "max    2.023000e+03  9.010000e+02  6.000000e+00  9.000000e+00  9.000000e+00   \n",
       "\n",
       "              secu3  \n",
       "count  1.251410e+06  \n",
       "mean  -9.200246e-01  \n",
       "std    8.557038e-01  \n",
       "min   -1.000000e+00  \n",
       "25%   -1.000000e+00  \n",
       "50%   -1.000000e+00  \n",
       "75%   -1.000000e+00  \n",
       "max    9.000000e+00  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cae964-794e-4a19-a5c3-813fd85920a5",
   "metadata": {},
   "source": [
    "### Remove full row duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df30dc4-bd63-4869-9d98-2a0b33a4ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4644\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.duplicated().sum())\n",
    "merged_df = merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f4610-efde-438a-a277-a1018d50a695",
   "metadata": {},
   "source": [
    "### Adjustment of some column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "881d174f-d5cc-470e-a4b3-1a2d54602ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"lum\", \"agg\", \"int\", \"atm\", \"col\", \"catr\", \"circ\", \"prof\", \"plan\", \"surf\",\n",
    "    \"infra\", \"situ\", \"env1\", \"senc\", \"catv\", \"catu\", \"grav\", \"sexe\", \"trajet\",\n",
    "    \"secu\", \"locp\", \"etatp\", \"vosp\", \"obs\", \"obsm\", \"choc\", \"manv\", \"motor\",\n",
    "    \"secu1\", \"secu2\", \"secu3\", \"place\", \"nbv\", \"actp\", \"pr\", \"pr1\"\n",
    "]\n",
    "\n",
    "string_columns = [\n",
    "    \"com\", \"dep\", \"voie\", \"v2\", \"adr\", \"larrout\",\n",
    "    \"lat\", \"long\", \"gps\", \"num_veh\", \"num_veh_usag\", \"id_vehicule\",\n",
    "    \"id_vehicule_usag\", \"id_usager\"\n",
    "]\n",
    "\n",
    "int_columns = [\"an\", \"mois\", \"jour\", \"an_nais\", \"vma\", \"num_acc\", \"secu1\", \"secu2\", \"secu3\", \"secu\"]\n",
    "time_columns = [\"hrmn\"]\n",
    "\n",
    "# Apply conversions\n",
    "for col in categorical_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].astype(\"category\")\n",
    "\n",
    "for col in string_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].astype(\"string\")\n",
    "\n",
    "for col in int_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce').astype(\"Int64\")\n",
    "\n",
    "for col in time_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].astype(str)\n",
    "\n",
    "# Convert lartpc to float\n",
    "if \"lartpc\" in merged_df.columns:\n",
    "    merged_df[\"lartpc\"] = pd.to_numeric(merged_df[\"lartpc\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851d681-295b-4f5a-be03-ea142560cbe0",
   "metadata": {},
   "source": [
    "### Creating daytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233482c9-7861-4a2f-838a-421d02782ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_hrmn(val):\n",
    "    if pd.isna(val):\n",
    "        return \"0000\"\n",
    "    \n",
    "    val = str(val).strip()\n",
    "    \n",
    "    # Format: HH:MM\n",
    "    if re.match(r\"^\\d{1,2}:\\d{2}$\", val):\n",
    "        return val.replace(\":\", \"\").zfill(4)\n",
    "    \n",
    "    # Format: HHMM\n",
    "    if val.isdigit() and len(val) in [3, 4]:\n",
    "        return val.zfill(4)\n",
    "    \n",
    "    # Minute-only entry like \"45\" -> \"0045\"\n",
    "    if val.isdigit() and len(val) <= 2:\n",
    "        return \"00\" + val.zfill(2)\n",
    "    \n",
    "    # Invalid or unrecognized format\n",
    "    return \"0000\"\n",
    "\n",
    "merged_df[\"hrmn_clean\"] = merged_df[\"hrmn\"].apply(clean_hrmn)\n",
    "\n",
    "merged_df[\"datetime\"] = pd.to_datetime(\n",
    "    merged_df[\"an\"].astype(str) + \"-\" +\n",
    "    merged_df[\"mois\"].astype(str).str.zfill(2) + \"-\" +\n",
    "    merged_df[\"jour\"].astype(str).str.zfill(2) + \" \" +\n",
    "    merged_df[\"hrmn_clean\"].str[:2] + \":\" + merged_df[\"hrmn_clean\"].str[2:],\n",
    "    format=\"%Y-%m-%d %H:%M\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "merged_df[\"hour\"] = merged_df[\"datetime\"].dt.hour.astype(\"Int64\")\n",
    "merged_df[\"dayofweek\"] = merged_df[\"datetime\"].dt.dayofweek.astype(\"Int64\")\n",
    "merged_df[\"weekday_name\"] = merged_df[\"datetime\"].dt.day_name().astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140c894-404b-4b4a-a50e-20839e6466ba",
   "metadata": {},
   "source": [
    "### Adjust coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c95e76f-0200-4b64-ac74-c18bb55c47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize value:\n",
    "# - if string with comma, convert to float\n",
    "# - if integer-looking, shift decimal\n",
    "def normalize_gps_value(val):\n",
    "    try:\n",
    "        val_str = str(val).strip()\n",
    "        # Case 1: comma decimal (e.g., \"45,73306000\")\n",
    "        if ',' in val_str:\n",
    "            return float(val_str.replace(\",\", \".\"))\n",
    "        # Case 2: integer-style needing decimal shift\n",
    "        val_float = float(val_str)\n",
    "        if val_float > 1e5:\n",
    "            shifted = str(int(val_float)).zfill(7)\n",
    "            return float(shifted[:2] + '.' + shifted[2:])\n",
    "        return val_float\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply normalization\n",
    "merged_df[\"lat_clean\"] = merged_df[\"lat\"].apply(normalize_gps_value)\n",
    "merged_df[\"long_clean\"] = merged_df[\"long\"].apply(normalize_gps_value)\n",
    "\n",
    "# Build GPS string, return NaN if either value is 0.0\n",
    "def combine_gps(row):\n",
    "    lat = row[\"lat_clean\"]\n",
    "    lon = row[\"long_clean\"]\n",
    "    if pd.notna(lat) and pd.notna(lon) and not (lat == 0.0 and lon == 0.0):\n",
    "        return f\"{lat:.5f},{lon:.5f}\"\n",
    "    return np.nan\n",
    "\n",
    "merged_df[\"gps_combined\"] = merged_df.apply(combine_gps, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67703df5-e7b1-4f42-97e8-50468daa1f1c",
   "metadata": {},
   "source": [
    "### Estimation of the missing GPS: Combined Location Strategy (GPS + Administrative Fallback)\n",
    "\n",
    "To ensure comprehensive location coverage in the dataset, I apply a **combined strategy** that uses GPS data when available, and falls back to administrative centroids (`com` / `dep`) when GPS is missing.\n",
    "\n",
    "**1. Use GPS Where Available**  \n",
    "Leverage the existing `gps_combined`, `lat`, and `long` columns. These values are used directly when present and valid.\n",
    "\n",
    "**2. Fill Missing Locations Using Commune Centroids**  \n",
    "When GPS data is missing, we use the `dep` (department code) and `com` (commune code) to retrieve the geographic **centroid of the municipality**. This provides a reasonable location approximation.\n",
    "\n",
    "To support this strategy, a **commune centroid dataset** is needed ([20230823-communes-departement-region.csv](https://www.data.gouv.fr/en/datasets/communes-de-france-base-des-codes-postaux/))\n",
    "\n",
    "This file includes:\n",
    "\n",
    "- `code_commune_INSEE` â†’ Commune code (`com`)\n",
    "- `code_departement` â†’ Department code (`dep`)\n",
    "- `nom_commune` â†’ Name of the commune\n",
    "- `latitude` â†’ Latitude of the commune centroid\n",
    "- `longitude` â†’ Longitude of the commune centroid\n",
    "\n",
    "\n",
    "This dataset enables geolocation of accident records that lack GPS coordinates, ensuring that every row has a spatial reference for mapping and analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d184b53-621d-4d86-b228-3b17a3609746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dep   com insee_code\n",
      "0  590  11.0      59011\n",
      "1  590  11.0      59011\n",
      "2  590  11.0      59011\n",
      "3  590  11.0      59011\n",
      "4  590  11.0      59011\n"
     ]
    }
   ],
   "source": [
    "# produce clean INSEE code \n",
    "\n",
    "merged_df[\"dep\"] = pd.to_numeric(merged_df[\"dep\"], errors=\"coerce\")\n",
    "merged_df[\"com\"] = pd.to_numeric(merged_df[\"com\"], errors=\"coerce\")\n",
    "\n",
    "def clean_dep(dep):\n",
    "    try:\n",
    "        dep_str = str(int(float(dep))).zfill(3)\n",
    "        if len(dep_str) > 2 and dep_str.endswith(\"0\"):\n",
    "            return dep_str[:2]\n",
    "        return dep_str\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def clean_com(com):\n",
    "    try:\n",
    "        return str(int(float(com)))[-3:].zfill(3)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def build_insee(row):\n",
    "    dep_clean = clean_dep(row[\"dep\"])\n",
    "    com_clean = clean_com(row[\"com\"])\n",
    "    if pd.notna(dep_clean) and pd.notna(com_clean):\n",
    "        return dep_clean + com_clean\n",
    "    return np.nan\n",
    "\n",
    "merged_df[\"insee_code\"] = merged_df.apply(build_insee, axis=1)\n",
    "\n",
    "print(merged_df[[\"dep\", \"com\", \"insee_code\"]].head())\n",
    "\n",
    "#Create a Paris INSEE code correction map\n",
    "paris_fix = {f\"750{str(i).zfill(2)}\": f\"751{str(i).zfill(2)}\" for i in range(1, 21)}\n",
    "merged_df[\"insee_code\"] = merged_df[\"insee_code\"].replace(paris_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd776e8-f89e-49cf-85c2-1f58c45f1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final GPS coverage (original or centroid): 96.90%\n"
     ]
    }
   ],
   "source": [
    "centroids = pd.read_csv(\n",
    "    \"../data/raw/20230823-communes-departement-region.csv\",\n",
    "    sep=\",\", encoding=\"utf-8\",\n",
    "    usecols=[\"code_commune_INSEE\", \"latitude\", \"longitude\"]\n",
    ")\n",
    "\n",
    "centroids.rename(columns={\n",
    "    \"code_commune_INSEE\": \"insee_code\",\n",
    "    \"latitude\": \"lat_centroid\",\n",
    "    \"longitude\": \"long_centroid\"\n",
    "}, inplace=True)\n",
    "\n",
    "centroids[\"insee_code\"] = centroids[\"insee_code\"].astype(str).str.zfill(5)\n",
    "\n",
    "merged_df = merged_df.merge(centroids, how=\"left\", on=\"insee_code\")\n",
    "\n",
    "merged_df[\"gps_combined\"] = merged_df[\"gps_combined\"].astype(str).replace(\"nan\", np.nan)\n",
    "\n",
    "# Use original gps_combined if present; otherwise, use centroid fallback\n",
    "def choose_best_gps(row):\n",
    "    if pd.notna(row.get(\"gps_combined\")) and row[\"gps_combined\"] != \"nan\":\n",
    "        return row[\"gps_combined\"]\n",
    "    elif pd.notna(row.get(\"lat_centroid\")) and pd.notna(row.get(\"long_centroid\")):\n",
    "        return f\"{row['lat_centroid']},{row['long_centroid']}\"\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "merged_df[\"gps_combined_final\"] = merged_df.apply(choose_best_gps, axis=1)\n",
    "\n",
    "# Coverage report\n",
    "coverage = merged_df[\"gps_combined_final\"].notna().mean() * 100\n",
    "print(f\"âœ… Final GPS coverage (original or centroid): {coverage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4c7db-ed2b-424d-bb8e-d2d5f387f355",
   "metadata": {},
   "source": [
    "### Safety Equipment (`secu`, `secu1`, `secu2`, `secu3`)\n",
    "\n",
    "These columns describe the presence and usage of safety equipment by road users in accidents. The format differs before and after **2019**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Before 2019 â€” `secu` (2-digit code)\n",
    "\n",
    "- `secu` is a 2-character code:\n",
    "  - **First digit** = type of equipment\n",
    "  - **Second digit** = usage status  \n",
    "    `1 = used`, `2 = not used`, `3 = not determinable`\n",
    "\n",
    "Only codes ending in `1` (used) are retained for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### From 2019 â€” `secu1`, `secu2`, `secu3`\n",
    "\n",
    "These represent up to three types of safety equipment used per user.\n",
    "\n",
    "Each code is a single integer:\n",
    "\n",
    "| Code | Meaning                              |\n",
    "|------|--------------------------------------|\n",
    "| -1   | Not specified                        |\n",
    "| 0    | No equipment                         |\n",
    "| 1    | Belt                                 |\n",
    "| 2    | Helmet                               |\n",
    "| 3    | Child safety device                  |\n",
    "| 4    | Reflective vest                      |\n",
    "| 5    | Airbag (2/3-wheel vehicles)          |\n",
    "| 6    | Gloves (2/3-wheel vehicles)          |\n",
    "| 7    | Gloves + Airbag (2/3-wheel vehicles) |\n",
    "| 8    | Not determinable                     |\n",
    "| 9    | Other                                |\n",
    "\n",
    "Codes `-1`, and `8` are treated as **missing or invalid**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- Summary columns are created for key items:  \n",
    "  `belt_status`, `helmet_status`, `child_device_status`, `reflective_vest_status`  \n",
    "  with values:\n",
    "  - `1` â†’ used\n",
    "  - `0` â†’ not used\n",
    "  - `-1` â†’ not specified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77f6109a-2c4b-4cca-ae81-4220abd129d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equipment_status(row, target_code):\n",
    "    # === Post-2019 ===\n",
    "    if not pd.isna(row.get(\"secu1\")):\n",
    "        values = [row.get(f\"secu{i}\") for i in range(1, 4)]\n",
    "        if any(v == target_code for v in values):\n",
    "            return 1  # used\n",
    "        if all(v in [-1, 8, None, np.nan] for v in values):\n",
    "            return -1  # not specified\n",
    "        return 0  # not used\n",
    "\n",
    "    # === Pre-2019 ===\n",
    "    secu_val = row.get(\"secu\")\n",
    "    if pd.notna(secu_val):\n",
    "        s = str(secu_val).strip()\n",
    "        if len(s) == 2 and s[0] == str(target_code):\n",
    "            if s[1] == \"1\":\n",
    "                return 1\n",
    "            elif s[1] == \"2\":\n",
    "                return 0\n",
    "            elif s[1] == \"3\":\n",
    "                return -1\n",
    "    return -1\n",
    "    \n",
    "merged_df[\"belt_status\"] = merged_df.apply(get_equipment_status, target_code=1, axis=1)\n",
    "merged_df[\"helmet_status\"] = merged_df.apply(get_equipment_status, target_code=2, axis=1)\n",
    "merged_df[\"child_device_status\"] = merged_df.apply(get_equipment_status, target_code=3, axis=1)\n",
    "merged_df[\"reflective_vest_status\"] = merged_df.apply(get_equipment_status, target_code=4, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae647e-2127-47bb-943c-e9c65dbfe39f",
   "metadata": {},
   "source": [
    "### Check the data balance and calculate the proportion of the most common value per column. \n",
    "Flag as \"Unbalanced\" if that value exceeds 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76488e18-7435-4e89-a311-aa31d1791776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Column  Most Frequent Value %     Balance  \\\n",
      "0                  num_acc                   0.12    Balanced   \n",
      "1                       an                   7.81    Balanced   \n",
      "2                     mois                   9.52    Balanced   \n",
      "3                     jour                   3.46    Balanced   \n",
      "4                     hrmn                   1.34    Balanced   \n",
      "5                      lum                  69.97    Balanced   \n",
      "6                      agg                  66.10    Balanced   \n",
      "7                      int                  68.28    Balanced   \n",
      "8                      atm                  81.67  Unbalanced   \n",
      "9                      col                  33.24    Balanced   \n",
      "10                     com                   4.92    Balanced   \n",
      "11                     adr                  10.72    Balanced   \n",
      "12                     gps                  61.60    Balanced   \n",
      "13                     lat                  45.71    Balanced   \n",
      "14                    long                  45.71    Balanced   \n",
      "15                     dep                   7.04    Balanced   \n",
      "16                    catr                  49.33    Balanced   \n",
      "17                    voie                  40.50    Balanced   \n",
      "18                      v1                  53.97    Balanced   \n",
      "19                      v2                  95.63  Unbalanced   \n",
      "20                    circ                  59.50    Balanced   \n",
      "21                     nbv                  50.07    Balanced   \n",
      "22                      pr                  43.40    Balanced   \n",
      "23                     pr1                  43.54    Balanced   \n",
      "24                    vosp                  92.05  Unbalanced   \n",
      "25                    prof                  77.97  Unbalanced   \n",
      "26                    plan                  80.75  Unbalanced   \n",
      "27                  lartpc                  64.63    Balanced   \n",
      "28                 larrout                  18.42    Balanced   \n",
      "29                    surf                  79.71  Unbalanced   \n",
      "30                   infra                  87.24  Unbalanced   \n",
      "31                    situ                  89.16  Unbalanced   \n",
      "32                    env1                  47.71    Balanced   \n",
      "33                    senc                  71.37  Unbalanced   \n",
      "34                    catv                  66.03    Balanced   \n",
      "35                  occutc                  82.62  Unbalanced   \n",
      "36                     obs                  89.84  Unbalanced   \n",
      "37                    obsm                  73.52  Unbalanced   \n",
      "38                    choc                  36.90    Balanced   \n",
      "39                    manv                  42.75    Balanced   \n",
      "40                 num_veh                  52.12    Balanced   \n",
      "41                   place                  78.23  Unbalanced   \n",
      "42                    catu                  78.15  Unbalanced   \n",
      "43                    grav                  44.66    Balanced   \n",
      "44                    sexe                  66.88    Balanced   \n",
      "45                  trajet                  35.77    Balanced   \n",
      "46                    secu                  49.43    Balanced   \n",
      "47                    locp                  85.03  Unbalanced   \n",
      "48                    actp                  77.07  Unbalanced   \n",
      "49                   etatp                  77.10  Unbalanced   \n",
      "50                 an_nais                   2.56    Balanced   \n",
      "51            num_veh_usag                  51.39    Balanced   \n",
      "52                     vma                  83.38  Unbalanced   \n",
      "53             id_vehicule                  83.38  Unbalanced   \n",
      "54                   motor                  83.38  Unbalanced   \n",
      "55        id_vehicule_usag                  83.38  Unbalanced   \n",
      "56                   secu1                  83.38  Unbalanced   \n",
      "57                   secu2                  83.38  Unbalanced   \n",
      "58                   secu3                  83.38  Unbalanced   \n",
      "59               id_usager                  89.39  Unbalanced   \n",
      "60              hrmn_clean                   1.58    Balanced   \n",
      "61                datetime                   0.12    Balanced   \n",
      "62                    hour                   9.38    Balanced   \n",
      "63               dayofweek                  16.91    Balanced   \n",
      "64            weekday_name                  16.91    Balanced   \n",
      "65               lat_clean                  45.71    Balanced   \n",
      "66              long_clean                  45.97    Balanced   \n",
      "67            gps_combined                  54.36    Balanced   \n",
      "68              insee_code                   4.83    Balanced   \n",
      "69            lat_centroid                  20.51    Balanced   \n",
      "70           long_centroid                  20.51    Balanced   \n",
      "71      gps_combined_final                   4.35    Balanced   \n",
      "72             belt_status                  59.98    Balanced   \n",
      "73           helmet_status                  68.63    Balanced   \n",
      "74     child_device_status                  83.65  Unbalanced   \n",
      "75  reflective_vest_status                  84.12  Unbalanced   \n",
      "\n",
      "   Most Frequent Value  \n",
      "0                 None  \n",
      "1                 None  \n",
      "2                 None  \n",
      "3                 None  \n",
      "4                 None  \n",
      "5                 None  \n",
      "6                 None  \n",
      "7                 None  \n",
      "8                  1.0  \n",
      "9                 None  \n",
      "10                None  \n",
      "11                None  \n",
      "12                None  \n",
      "13                None  \n",
      "14                None  \n",
      "15                None  \n",
      "16                None  \n",
      "17                None  \n",
      "18                None  \n",
      "19                <NA>  \n",
      "20                None  \n",
      "21                None  \n",
      "22                None  \n",
      "23                None  \n",
      "24                 0.0  \n",
      "25                 1.0  \n",
      "26                 1.0  \n",
      "27                None  \n",
      "28                None  \n",
      "29                 1.0  \n",
      "30                 0.0  \n",
      "31                 1.0  \n",
      "32                None  \n",
      "33                 0.0  \n",
      "34                None  \n",
      "35                 0.0  \n",
      "36                 0.0  \n",
      "37                 2.0  \n",
      "38                None  \n",
      "39                None  \n",
      "40                None  \n",
      "41                 1.0  \n",
      "42                   1  \n",
      "43                None  \n",
      "44                None  \n",
      "45                None  \n",
      "46                None  \n",
      "47                 0.0  \n",
      "48                   0  \n",
      "49                 0.0  \n",
      "50                None  \n",
      "51                None  \n",
      "52                <NA>  \n",
      "53                <NA>  \n",
      "54                 NaN  \n",
      "55                <NA>  \n",
      "56                <NA>  \n",
      "57                <NA>  \n",
      "58                <NA>  \n",
      "59                <NA>  \n",
      "60                None  \n",
      "61                None  \n",
      "62                None  \n",
      "63                None  \n",
      "64                None  \n",
      "65                None  \n",
      "66                None  \n",
      "67                None  \n",
      "68                None  \n",
      "69                None  \n",
      "70                None  \n",
      "71                None  \n",
      "72                None  \n",
      "73                None  \n",
      "74                  -1  \n",
      "75                  -1  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "balance_all_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Column\": col,\n",
    "        \"Most Frequent Value %\": round((vc := merged_df[col].value_counts(normalize=True, dropna=False)).iloc[0] * 100, 2),\n",
    "        \"Balance\": \"Unbalanced\" if vc.iloc[0] > 0.7 else \"Balanced\",\n",
    "        \"Most Frequent Value\": vc.idxmax() if vc.iloc[0] > 0.7 else None\n",
    "    }\n",
    "    for col in merged_df.columns\n",
    "    if not merged_df[col].value_counts(normalize=True, dropna=False).empty\n",
    "])\n",
    "print(balance_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb824b-b596-4106-a2fd-51af438fac83",
   "metadata": {},
   "source": [
    "### Handling the missing data\n",
    "Check the missing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "858f2b1d-e6dc-40d4-b478-c4f386ad63dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_acc                    0.00\n",
      "an                         0.00\n",
      "mois                       0.00\n",
      "jour                       0.00\n",
      "hrmn                       0.00\n",
      "lum                        0.00\n",
      "agg                        0.00\n",
      "int                        0.00\n",
      "atm                        0.00\n",
      "col                        0.00\n",
      "com                        0.17\n",
      "adr                       10.72\n",
      "gps                       61.60\n",
      "lat                       45.71\n",
      "long                      45.71\n",
      "dep                        0.17\n",
      "catr                       0.00\n",
      "voie                       8.20\n",
      "v1                        53.97\n",
      "v2                        95.63\n",
      "circ                       0.13\n",
      "nbv                        0.25\n",
      "pr                        43.40\n",
      "pr1                       43.54\n",
      "vosp                       0.22\n",
      "prof                       0.16\n",
      "plan                       0.19\n",
      "lartpc                    21.25\n",
      "larrout                    7.84\n",
      "surf                       0.16\n",
      "infra                      0.40\n",
      "situ                       0.38\n",
      "env1                      17.02\n",
      "senc                       0.01\n",
      "catv                       0.00\n",
      "occutc                    16.44\n",
      "obs                        0.09\n",
      "obsm                       0.04\n",
      "choc                       0.03\n",
      "manv                       0.06\n",
      "num_veh                    0.00\n",
      "place                      2.68\n",
      "catu                       0.00\n",
      "grav                       0.00\n",
      "sexe                       0.00\n",
      "trajet                     0.02\n",
      "secu                      17.77\n",
      "locp                       2.53\n",
      "actp                       2.53\n",
      "etatp                      2.53\n",
      "an_nais                    0.31\n",
      "num_veh_usag               0.00\n",
      "vma                       83.38\n",
      "id_vehicule               83.38\n",
      "motor                     83.38\n",
      "id_vehicule_usag          83.38\n",
      "secu1                     83.38\n",
      "secu2                     83.38\n",
      "secu3                     83.38\n",
      "id_usager                 89.39\n",
      "hrmn_clean                 0.00\n",
      "datetime                   0.00\n",
      "hour                       0.00\n",
      "dayofweek                  0.00\n",
      "weekday_name               0.00\n",
      "lat_clean                 45.71\n",
      "long_clean                45.97\n",
      "gps_combined              54.36\n",
      "insee_code                 0.17\n",
      "lat_centroid              20.51\n",
      "long_centroid             20.51\n",
      "gps_combined_final         3.10\n",
      "belt_status                0.00\n",
      "helmet_status              0.00\n",
      "child_device_status        0.00\n",
      "reflective_vest_status     0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Replace actual NA values and string 'nan' with np.nan\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "merged_df = merged_df.apply(lambda col: col.map(lambda x: np.nan if pd.isna(x) or str(x).strip().lower() == \"nan\" else x))\n",
    "\n",
    "missing_percent = merged_df.isnull().mean().multiply(100).round(2)\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716744a-4e7b-445a-a230-7d58b933704c",
   "metadata": {},
   "source": [
    "Drop some categories with many NaN fields that cannot be used for the further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e2bfba5-a333-46e4-8cff-8cb856b495b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: ['num_acc', 'an', 'mois', 'jour', 'hrmn', 'lum', 'agg', 'int', 'atm', 'col', 'catr', 'voie', 'circ', 'nbv', 'vosp', 'prof', 'plan', 'larrout', 'surf', 'infra', 'situ', 'senc', 'catv', 'obs', 'obsm', 'choc', 'manv', 'num_veh', 'place', 'catu', 'grav', 'sexe', 'trajet', 'locp', 'actp', 'etatp', 'an_nais', 'hrmn_clean', 'datetime', 'hour', 'dayofweek', 'weekday_name', 'insee_code', 'gps_combined_final', 'belt_status', 'helmet_status', 'child_device_status', 'reflective_vest_status']\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    \"com\", \"adr\", \"gps\", \"lat\", \"long\", \"dep\", \"v1\", \"v2\", \"pr\", \"pr1\", \"lartpc\",\n",
    "    \"env1\", \"occutc\", \"secu\", \"num_veh_usag\", \"vma\", \"id_vehicule\", \"motor\",\n",
    "    \"id_vehicule_usag\", \"secu1\", \"secu2\", \"secu3\", \"id_usager\", \"lat_clean\",\n",
    "    \"long_clean\", \"gps_combined\", \"lat_centroid\", \"long_centroid\"\n",
    "]\n",
    "\n",
    "merged_df = merged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns])\n",
    "\n",
    "print(\"Remaining columns:\", merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5799650-fb22-4595-ad69-959dc0e2993c",
   "metadata": {},
   "source": [
    "### Extract processed csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70fca175-a24b-4d01-be00-2e9cc19b6e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed data saved to: ../data/processed/accidents_processed.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../data/processed/accidents_processed.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Processed data saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
