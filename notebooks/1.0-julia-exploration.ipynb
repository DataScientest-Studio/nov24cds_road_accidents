{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b06d9ac-fa77-4265-890d-086c4d0c5a3e",
   "metadata": {},
   "source": [
    "# Project: Road Accidents in France based on Annual Road Traffic Accident Injury Database (2005 - 2023)\n",
    "\n",
    "## Step 1: Exploration + Data Visualization\n",
    "\n",
    "---\n",
    "\n",
    "### Description\n",
    "\n",
    "For each accident involving injury (i.e., an accident occurring on a road open to public traffic, involving at least one vehicle and resulting in at least one victim requiring medical treatment), information describing the accident is recorded by the law enforcement unit (police, gendarmerie, etc.) that responded to the scene. These entries are compiled into a form called the *Injury Accident Analysis Bulletin*. Together, these forms constitute the national road traffic accident file, known as the **BAAC File**, administered by the **National Interministerial Road Safety Observatory (ONISR)**.\n",
    "\n",
    "The databases, extracted from the BAAC file, list all road traffic accidents involving injuries that occurred during a given year in:\n",
    "\n",
    "- Mainland France\n",
    "- Overseas departments (Guadeloupe, French Guiana, Martinique, Réunion, and Mayotte since 2012)\n",
    "- Overseas territories (Saint-Pierre and Miquelon, Saint-Barthélemy, Saint-Martin, Wallis and Futuna, French Polynesia, and New Caledonia; available since 2019)\n",
    "\n",
    "The databases from 2005 to 2023 are structured annually and consist of four CSV files:\n",
    "\n",
    "- **Caractéristiques** (Accident details)\n",
    "- **Lieux** (Locations)\n",
    "- **Véhicules** (Vehicles)\n",
    "- **Usagers** (Users)\n",
    "\n",
    "---\n",
    "\n",
    "The objective of this project is to try to predict the severity of road accidents in France. Predictions will be based on historical data.\n",
    "A first step is to study and apply methods to clean the dataset. Once the dataset is clean, a second step is to extract from the match history the characteristics that seem to be relevant for estimating the severity of accidents. Then, from its results, the objective is to work on a scoring of the risk zones according to the meteorological information, the geographical location (GPS coordinates, satellite images, …).\n",
    "Once the model has been trained, we will compare our model with historical data.\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- **Runaway Users:**  \n",
    "  Since 2021, data on runaway users has been added.  \n",
    "  This results in missing information such as sex, age, and injury severity (unharmed, slightly injured, hospitalized).\n",
    "\n",
    "- **Missing Data:**  \n",
    "  Most variables across the four main files may contain:\n",
    "  - Empty cells\n",
    "  - Zeros\n",
    "  - Periods (`.`)\n",
    "In these cases, the field was either not populated by law enforcement or the information was deemed irrelevant.\n",
    "Some categories use -1 to indicate 'not specified'.\n",
    "\n",
    "- **Hospitalized Injured Persons:**  \n",
    "  - Data regarding the classification of hospitalized injured persons since 2018 cannot be compared with previous years due to changes in the law enforcement data entry process. The \"hospitalized injured person\" indicator has not been certified by the Public Statistics Authority since 2019.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Handling\n",
    "\n",
    "I downloaded the data from:  \n",
    "[Data Source - data.gouv.fr](https://www.data.gouv.fr/en/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2023/)  \n",
    "and saved it into the `data/raw/` folder.\n",
    "\n",
    "Each year includes four main tables:\n",
    "\n",
    "- **Caractéristiques** (Caracteristics, or Accident details)\n",
    "- **Lieux** (Locations)\n",
    "- **Véhicules** (Vehicles)\n",
    "- **Usagers** (Users)\n",
    "\n",
    "Additionally, data on **registered vehicles (vehicules-immatricules-baac)** for 2009–2022 is available.  \n",
    "However, I chose not to use this dataset for now, as it may introduce problems in machine learning algorithms due to missing years.\n",
    "\n",
    "To streamline data processing, I corrected the names of some downloaded files to follow the format:  \n",
    "`category_year` (e.g., `Caracteristiques_2020.csv`, `Vehicules_2018.csv`).\n",
    "\n",
    "---\n",
    "\n",
    "The file columns contain encoded information about various accident characteristics.  \n",
    "The descriptions are provided in the latest version of the encoding documentation:\n",
    "\n",
    "- **description-des-bases-de-donnees-annuelles.pdf** (description of annual databases)\n",
    "- **Caracteristics.docx** (translated characteristics from French to English)\n",
    "\n",
    "Both documents are stored in the `references/` folder.\n",
    "\n",
    "Short description of the columns \n",
    "| Column                | Description |\n",
    "|-----------------------|-----------------------------------|\n",
    "| **num_acc**           | Unique identifier of the accident, assigned by the law enforcement. Join key between the files characteristics, locations, vehicles and users. |\n",
    "| **an**                | Year of the accident. |\n",
    "| **mois**              | Month of the accident. |\n",
    "| **jour**              | Day of the accident. |\n",
    "| **hrmn**              | Hour and minutes of the accident (format hhmm). |\n",
    "| **lum**               | Lighting conditions at the time of the accident: 1 = Full daylight, 2 = Twilight or dawn, 3 = Night without public lighting, 4 = Night with public lighting not lit, 5 = Night with public lighting lit. |\n",
    "| **agg**               | Urban area indicator: 1 = Outside urban area, 2 = Inside urban area. |\n",
    "| **int**               | Type of intersection: 1 = Outside intersection, 2 = Intersection in X, 3 = Intersection in T, 4 = Intersection in Y, 5 = More than 4 branches, 6 = Roundabout, 7 = Square, 8 = Level crossing, 9 = Other intersection. |\n",
    "| **atm**               | Atmospheric conditions at the time of the accident: -1 = Not specified, 1 = Normal, 2 = Light rain, 3 = Heavy rain, 4 = Snow–hail, 5 = Fog–smoke, 6 = Strong wind–storm, 7 = Dazzling, 8 = Overcast, 9 = Other. |\n",
    "| **col**               | Type of collision: -1 = Not specified, 1 = Two vehicles – head-on, 2 = Two vehicles – rear-end, 3 = Two vehicles – side, 4 = Three vehicles or more – in chain, 5 = Three vehicles or more – multiple collisions, 6 = Other collision, 7 = Without collision. |\n",
    "| **com**               | INSEE code of the municipality of the accident. |\n",
    "| **adr**               | Postal address of the accident, when the accident is located inside urban area. |\n",
    "| **gps**               | GPS coordinates (raw text). |\n",
    "| **lat**               | Latitude (decimal degrees). |\n",
    "| **long**              | Longitude (decimal degrees). |\n",
    "| **dep**               | INSEE department code of the accident. |\n",
    "| **catr**              | Road category: 1 = Highway, 2 = National road, 3 = Departmental road, 4 = Communal road, 5 = Outside public network, 6 = Parking lot, 7 = Urban metropolitan road, 9 = Other. |\n",
    "| **voie**              | Name or number of the road at the place of the accident. |\n",
    "| **v1**                | Numeric sub-identifier for the road (e.g., \"2 bis\" expressed as number). |\n",
    "| **v2**                | Alphanumeric sub-identifier for the road. |\n",
    "| **circ**              | Traffic regime: -1 = Not specified, 1 = One way, 2 = Two way, 3 = Separated carriageways, 4 = Variable. |\n",
    "| **nbv**               | Number of lanes for vehicles at the section of the road where the accident happened. |\n",
    "| **pr**                | PR number (reference point, upstream terminal) — -1 if not filled. |\n",
    "| **pr1**               | Distance to PR in meters (relative to the upstream terminal) — -1 if not filled. |\n",
    "| **vosp**              | Reserved lane: -1 = Not specified, 0 = Not applicable, 1 = Cycle track, 2 = Cycle lane, 3 = Reserved lane. |\n",
    "| **prof**              | Longitudinal profile of the road: -1 = Not specified, 1 = Flat, 2 = Slope, 3 = Top of hill, 4 = Bottom of hill. |\n",
    "| **plan**              | Road plan: -1 = Not specified, 1 = Straight, 2 = Left curve, 3 = Right curve, 4 = \"S\" curve. |\n",
    "| **lartpc**            | Width of the central reservation (in meters). |\n",
    "| **larrout**           | Width of the carriageway assigned to vehicles (in meters). |\n",
    "| **surf**              | Surface condition: -1 = Not specified, 1 = Normal, 2 = Wet, 3 = Puddles, 4 = Flooded, 5 = Snow, 6 = Mud, 7 = Icy, 8 = Fatty/oily, 9 = Other. |\n",
    "| **infra**             | Infrastructure: -1 = Not specified, 0 = None, 1 = Tunnel, 2 = Bridge, 3 = Interchange or ramp, 4 = Railway, 5 = Crossing, 6 = Pedestrian area, 7 = Toll zone, 8 = Construction, 9 = Other. |\n",
    "| **situ**              | Situation: -1 = Not specified, 0 = None, 1 = On roadway, 2 = On emergency lane, 3 = On shoulder, 4 = On sidewalk, 5 = On cycle path, 6 = On special route, 8 = Other. |\n",
    "| **env1**              | Environment at the site of the accident (additional codings, see documentation if available). |\n",
    "| **senc**              | Direction of movement: -1 = Not specified, 0 = Unknown, 1 = Ascending, 2 = Descending, 3 = No reference. |\n",
    "| **catv**              | Category of vehicle involved: 01 = Bicycle, 07 = Passenger car, 13 = Heavy truck, 31 = Motorcycle, 37 = Bus, etc. (see full coding in documentation). |\n",
    "| **occutc**            | Number of people present in the vehicle (for public transport vehicles). |\n",
    "| **obs**               | Fixed obstacle hit: -1 = Not specified, 0 = Not applicable, 1 = Parked vehicle, 2 = Tree, 3 = Post, 4 = Rail guard, 5 = Concrete wall, 6 = Building, 7 = Fire hydrant, 8 = Lamp post, 9 = Other. |\n",
    "| **obsm**              | Mobile obstacle hit: -1 = Not specified, 0 = None, 1 = Pedestrian, 2 = Vehicle, 3 = Animal, 4 = Other. |\n",
    "| **choc**              | Initial point of impact: -1 = Not specified, 0 = None, 1 = Front, 2 = Front right, 3 = Front left, 4 = Rear, 5 = Rear right, 6 = Rear left, 7 = Side right, 8 = Side left. |\n",
    "| **manv**              | Main maneuver before the accident: -1 = Not specified, 1 = No change, 2 = Stopped, 3 = Stationary, 4 = Reversing, 5 = Parking, 6 = Starting, 7 = Overtaking right, 8 = Overtaking left, 9 = Changing lanes, 10 = U-turn, 11 = Turning right, 12 = Turning left, 13 = Other. |\n",
    "| **num_veh**           | Vehicle identifier in the accident, allows linking with occupants and users. Alphanumeric code. |\n",
    "| **place**             | Position occupied by the user in the vehicle or accident: 10 = Pedestrian (see documentation for other positions/seats). |\n",
    "| **catu**              | Category of user: 1 = Driver, 2 = Passenger, 3 = Pedestrian. |\n",
    "| **grav**              | Severity of injury for the user: 1 = Unharmed, 2 = Killed, 3 = Hospitalized, 4 = Light injury. |\n",
    "| **sexe**              | Gender of the user: 1 = Male, 2 = Female. |\n",
    "| **trajet**            | Reason for the journey: -1 or 0 = Not specified, 1 = Home-work, 2 = Home-school, 3 = Shopping, 4 = Professional, 5 = Leisure, 9 = Other. |\n",
    "| **secu**              | Safety equipment used by the user. Up to 2018: 0 = None, 1 = Seat belt, 2 = Helmet, 3 = Child seat, 4 = Reflective vest, 5 = Other (see secu1/secu2/secu3 for 2019+). |\n",
    "| **locp**              | Location of the pedestrian: -1 = Not specified, 1 = On roadway, 2 = On shoulder, 3 = On crosswalk, 4 = On cycle path, 5 = On sidewalk, 6 = On special route, 7 = Other. |\n",
    "| **actp**              | Action of the pedestrian: -1 = Not specified, 0 = Not applicable, 1 = Heading toward the vehicle, 2 = Moving away, 3 = Crossing, 4 = Waiting, 5 = Playing, 6 = Working, 9 = Other. |\n",
    "| **etatp**             | Pedestrian's situation: -1 = Not specified, 1 = Alone, 2 = Accompanied, 3 = In group. |\n",
    "| **an_nais**           | Year of birth of the user involved in the accident. |\n",
    "| **num_veh_usag**      | Vehicle identifier (user table), alphanumeric code. |\n",
    "| **vma**               | Maximum speed limit at the location and time of the accident (in km/h). |\n",
    "| **id_vehicule**       | Unique identifier of the vehicle (in users and vehicles files). |\n",
    "| **motor**             | Engine type: -1 = Not specified, 1 = Internal combustion, 2 = Hybrid, 3 = Electric, 4 = Hydrogen, 5 = Other. |\n",
    "| **id_vehicule_usag**  | Unique vehicle identifier from the users file. |\n",
    "| **secu1**             | 1st safety equipment used (from 2019 onwards). |\n",
    "| **secu2**             | 2nd safety equipment used (from 2019 onwards). |\n",
    "| **secu3**             | 3rd safety equipment used (from 2019 onwards). |\n",
    "| **id_usager**         | Unique identifier of the user (in the users file). |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "To prepare the data for machine learning, all the individual files must be merged into a single dataset. The Num_Acc column acts as the main key, allowing us to join the Caractéristiques, Lieux, Véhicules, and Usagers tables for each accident record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1f7cff-f145-4aa4-855d-34e822b1954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the packages \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e40a180-9c84-4b6b-829a-d746acc83139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as: ../data/processed\\accidents_merged_2005_2023.csv\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"../data/raw\"\n",
    "processed_path = \"../data/processed\"\n",
    "output_file = os.path.join(processed_path, \"accidents_merged_2005_2023.csv\")\n",
    "\n",
    "#Some files have different delimiter and a typical French encoder ISO-8859-1\n",
    "def robust_read_csv(fpath, encoding=\"ISO-8859-1\"): \n",
    "    for delim in [\",\", \";\", \"\\t\"]:\n",
    "        df = pd.read_csv(fpath, delimiter=delim, encoding=encoding, on_bad_lines=\"skip\", low_memory=False)\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace('\"', '')\n",
    "        # Rename for accident_id\n",
    "        if 'accident_id' in df.columns:\n",
    "            df = df.rename(columns={'accident_id': 'num_acc'}) #some datasets had different name for the num_acc key column\n",
    "        if 'num_acc' in df.columns:\n",
    "            return df\n",
    "    print(f\"!! Could not properly split columns for {fpath}, got: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "all_years = []\n",
    "for year in range(2005, 2024):\n",
    "    files = {\n",
    "        \"carac\": os.path.join(raw_path, f\"caracteristiques_{year}.csv\"),\n",
    "        \"lieux\": os.path.join(raw_path, f\"lieux_{year}.csv\"),\n",
    "        \"vehic\": os.path.join(raw_path, f\"vehicules_{year}.csv\"),\n",
    "        \"usag\": os.path.join(raw_path, f\"usagers_{year}.csv\"),\n",
    "    }\n",
    "    dfs = {k: robust_read_csv(fpath) for k, fpath in files.items()}\n",
    "\n",
    "    if all('num_acc' in df.columns for df in dfs.values()):\n",
    "        df = dfs[\"carac\"].merge(dfs[\"lieux\"], on=\"num_acc\", how=\"inner\", suffixes=('', '_lieux')) #Keep only rows that have a match in both DataFrames\n",
    "        df = df.merge(dfs[\"vehic\"], on=\"num_acc\", how=\"inner\", suffixes=('', '_vehic'))\n",
    "        df = df.merge(dfs[\"usag\"], on=\"num_acc\", how=\"inner\", suffixes=('', '_usag'))\n",
    "        df[\"an\"] = year  # Replace 'an' column with the current year for unification, for some years it was 5 instead of 2005\n",
    "        all_years.append(df)\n",
    "    else:\n",
    "        print(f\"Skipping year {year}: 'num_acc' not found in all files.\")\n",
    "\n",
    "# Combine all years and save\n",
    "merged_df = pd.concat(all_years, ignore_index=True)\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "print(f\"Merged file saved as: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70963b-7144-4bba-b529-19bb4402ff44",
   "metadata": {},
   "source": [
    "After a big merge like this, we need have a quick data health check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446ee84a-8ae5-428e-bf3c-2d2933d06847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5314184, 60)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>atm</th>\n",
       "      <th>col</th>\n",
       "      <th>...</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>num_veh_usag</th>\n",
       "      <th>vma</th>\n",
       "      <th>id_vehicule</th>\n",
       "      <th>motor</th>\n",
       "      <th>id_vehicule_usag</th>\n",
       "      <th>secu1</th>\n",
       "      <th>secu2</th>\n",
       "      <th>secu3</th>\n",
       "      <th>id_usager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>A01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200500000001</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1900</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>B02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acc    an  mois  jour  hrmn  lum  agg  int  atm  col  ... an_nais  \\\n",
       "0  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1976.0   \n",
       "1  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1968.0   \n",
       "2  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1964.0   \n",
       "3  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  2004.0   \n",
       "4  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1998.0   \n",
       "5  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1991.0   \n",
       "6  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1976.0   \n",
       "7  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1968.0   \n",
       "8  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  1964.0   \n",
       "9  200500000001  2005     1    12  1900    3    2    1  1.0  3.0  ...  2004.0   \n",
       "\n",
       "  num_veh_usag vma id_vehicule motor id_vehicule_usag  secu1 secu2  secu3  \\\n",
       "0          A01 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "1          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "2          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "3          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "4          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "5          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "6          A01 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "7          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "8          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "9          B02 NaN         NaN   NaN              NaN    NaN   NaN    NaN   \n",
       "\n",
       "  id_usager  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "5       NaN  \n",
       "6       NaN  \n",
       "7       NaN  \n",
       "8       NaN  \n",
       "9       NaN  \n",
       "\n",
       "[10 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(merged_df.shape)\n",
    "display(merged_df.head(10))\n",
    "#For the better understanding, I want to look at all columns, so I extract 100 random rows as a separate file\n",
    "#df = pd.read_csv(\"../data/processed/accidents_merged_2005_2023.csv\", low_memory=False)\n",
    "#df_sample = df.sample(n=100, random_state=42)\n",
    "#df_sample.to_csv(\"../data/processed/accidents_sample_100_random.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc51cdc-5fb2-4b44-9975-340b0cb02490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5314184 entries, 0 to 5314183\n",
      "Data columns (total 60 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   num_acc           int64  \n",
      " 1   an                int64  \n",
      " 2   mois              int64  \n",
      " 3   jour              int64  \n",
      " 4   hrmn              object \n",
      " 5   lum               int64  \n",
      " 6   agg               int64  \n",
      " 7   int               int64  \n",
      " 8   atm               float64\n",
      " 9   col               float64\n",
      " 10  com               object \n",
      " 11  adr               object \n",
      " 12  gps               object \n",
      " 13  lat               object \n",
      " 14  long              object \n",
      " 15  dep               object \n",
      " 16  catr              float64\n",
      " 17  voie              object \n",
      " 18  v1                float64\n",
      " 19  v2                object \n",
      " 20  circ              float64\n",
      " 21  nbv               object \n",
      " 22  pr                object \n",
      " 23  pr1               object \n",
      " 24  vosp              float64\n",
      " 25  prof              float64\n",
      " 26  plan              float64\n",
      " 27  lartpc            object \n",
      " 28  larrout           object \n",
      " 29  surf              float64\n",
      " 30  infra             float64\n",
      " 31  situ              float64\n",
      " 32  env1              float64\n",
      " 33  senc              float64\n",
      " 34  catv              int64  \n",
      " 35  occutc            float64\n",
      " 36  obs               float64\n",
      " 37  obsm              float64\n",
      " 38  choc              float64\n",
      " 39  manv              float64\n",
      " 40  num_veh           object \n",
      " 41  place             float64\n",
      " 42  catu              int64  \n",
      " 43  grav              int64  \n",
      " 44  sexe              int64  \n",
      " 45  trajet            float64\n",
      " 46  secu              float64\n",
      " 47  locp              float64\n",
      " 48  actp              object \n",
      " 49  etatp             float64\n",
      " 50  an_nais           float64\n",
      " 51  num_veh_usag      object \n",
      " 52  vma               float64\n",
      " 53  id_vehicule       object \n",
      " 54  motor             float64\n",
      " 55  id_vehicule_usag  object \n",
      " 56  secu1             float64\n",
      " 57  secu2             float64\n",
      " 58  secu3             float64\n",
      " 59  id_usager         object \n",
      "dtypes: float64(29), int64(11), object(20)\n",
      "memory usage: 2.4+ GB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f4610-efde-438a-a277-a1018d50a695",
   "metadata": {},
   "source": [
    "We need to adjust some column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "881d174f-d5cc-470e-a4b3-1a2d54602ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"lum\", \"agg\", \"int\", \"atm\", \"col\", \"catr\", \"circ\", \"prof\", \"plan\", \"surf\",\n",
    "    \"infra\", \"situ\", \"env1\", \"senc\", \"catv\", \"catu\", \"grav\", \"sexe\", \"trajet\",\n",
    "    \"secu\", \"locp\", \"etatp\", \"vosp\", \"obs\", \"obsm\", \"choc\", \"manv\", \"motor\",\n",
    "    \"secu1\", \"secu2\", \"secu3\", \"place\", \"nbv\", \"actp\", \"pr\", \"pr1\"\n",
    "]\n",
    "\n",
    "string_columns = [\n",
    "    \"com\", \"dep\", \"voie\", \"v2\", \"adr\", \"larrout\",\n",
    "    \"lat\", \"long\", \"gps\", \"num_veh\", \"num_veh_usag\", \"id_vehicule\",\n",
    "    \"id_vehicule_usag\", \"id_usager\"\n",
    "]\n",
    "\n",
    "int_columns = [\"an\", \"mois\", \"jour\", \"an_nais\", \"vma\", \"num_acc\"]\n",
    "time_columns = [\"hrmn\"]\n",
    "\n",
    "# Apply conversions\n",
    "for col in categorical_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].astype(\"category\")\n",
    "\n",
    "for col in string_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].astype(\"string\")\n",
    "\n",
    "for col in int_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce').astype(\"Int64\")\n",
    "\n",
    "for col in time_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].astype(str)\n",
    "\n",
    "# Convert lartpc to float\n",
    "if \"lartpc\" in merged_df.columns:\n",
    "    merged_df[\"lartpc\"] = pd.to_numeric(merged_df[\"lartpc\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851d681-295b-4f5a-be03-ea142560cbe0",
   "metadata": {},
   "source": [
    "Adjust daytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233482c9-7861-4a2f-838a-421d02782ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the cleaning function for hrmn\n",
    "def clean_hrmn(val):\n",
    "    if pd.isna(val):\n",
    "        return \"0000\"\n",
    "    \n",
    "    val = str(val).strip()\n",
    "    \n",
    "    # Format: HH:MM\n",
    "    if re.match(r\"^\\d{1,2}:\\d{2}$\", val):\n",
    "        return val.replace(\":\", \"\").zfill(4)\n",
    "    \n",
    "    # Format: HHMM\n",
    "    if val.isdigit() and len(val) in [3, 4]:\n",
    "        return val.zfill(4)\n",
    "    \n",
    "    # Minute-only entry like \"45\" -> \"0045\"\n",
    "    if val.isdigit() and len(val) <= 2:\n",
    "        return \"00\" + val.zfill(2)\n",
    "    \n",
    "    # Invalid or unrecognized format\n",
    "    return \"0000\"\n",
    "\n",
    "# Apply cleaning\n",
    "merged_df[\"hrmn_clean\"] = merged_df[\"hrmn\"].apply(clean_hrmn)\n",
    "\n",
    "# Create datetime\n",
    "merged_df[\"datetime\"] = pd.to_datetime(\n",
    "    merged_df[\"an\"].astype(str) + \"-\" +\n",
    "    merged_df[\"mois\"].astype(str).str.zfill(2) + \"-\" +\n",
    "    merged_df[\"jour\"].astype(str).str.zfill(2) + \" \" +\n",
    "    merged_df[\"hrmn_clean\"].str[:2] + \":\" + merged_df[\"hrmn_clean\"].str[2:],\n",
    "    format=\"%Y-%m-%d %H:%M\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Create time-related features\n",
    "merged_df[\"hour\"] = merged_df[\"datetime\"].dt.hour.astype(\"Int64\")\n",
    "merged_df[\"dayofweek\"] = merged_df[\"datetime\"].dt.dayofweek.astype(\"Int64\")\n",
    "merged_df[\"weekday_name\"] = merged_df[\"datetime\"].dt.day_name().astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ac54d-7107-400e-bcbc-e4258cce6e3f",
   "metadata": {},
   "source": [
    "Unique value counts per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92838f11-7454-487c-b517-c212c724873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "2005    374561\n",
      "2006    362507\n",
      "2007    356228\n",
      "2008    322196\n",
      "2009    311706\n",
      "2023    309341\n",
      "2010    288112\n",
      "2011    281675\n",
      "2012    263194\n",
      "2017    260392\n",
      "2016    257286\n",
      "2019    253488\n",
      "2014    248642\n",
      "2018    248406\n",
      "2021    248187\n",
      "2015    245706\n",
      "2013    242163\n",
      "2022    241487\n",
      "2020    198907\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['an'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd82ffa9-21c0-4ed0-a652-fd4a243fed21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acc</th>\n",
       "      <th>an</th>\n",
       "      <th>mois</th>\n",
       "      <th>jour</th>\n",
       "      <th>v1</th>\n",
       "      <th>lartpc</th>\n",
       "      <th>occutc</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>vma</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5314184.0</td>\n",
       "      <td>5314184.0</td>\n",
       "      <td>5314184.0</td>\n",
       "      <td>5314184.0</td>\n",
       "      <td>2.632973e+06</td>\n",
       "      <td>3.818966e+06</td>\n",
       "      <td>4.076093e+06</td>\n",
       "      <td>5292557.0</td>\n",
       "      <td>1251410.0</td>\n",
       "      <td>5314184</td>\n",
       "      <td>5314184.0</td>\n",
       "      <td>5314184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>201332794431.786469</td>\n",
       "      <td>2013.327611</td>\n",
       "      <td>6.701324</td>\n",
       "      <td>15.588233</td>\n",
       "      <td>-1.543806e-02</td>\n",
       "      <td>6.173533e+00</td>\n",
       "      <td>1.438132e-01</td>\n",
       "      <td>1975.503995</td>\n",
       "      <td>61.816122</td>\n",
       "      <td>2013-11-04 06:06:51.580249344</td>\n",
       "      <td>13.730389</td>\n",
       "      <td>2.984137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>200500000001.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1896.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2005-01-01 00:01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>200800056274.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2008-09-28 17:13:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>201300022821.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2013-06-11 22:00:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>201800041017.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2018-09-15 16:30:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>202300054822.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>9.600000e+02</td>\n",
       "      <td>9.000000e+02</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>2023-12-31 23:50:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>565902720.582649</td>\n",
       "      <td>5.65908</td>\n",
       "      <td>3.374824</td>\n",
       "      <td>8.759571</td>\n",
       "      <td>5.852336e-01</td>\n",
       "      <td>2.375976e+01</td>\n",
       "      <td>3.074271e+00</td>\n",
       "      <td>18.55589</td>\n",
       "      <td>26.977133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.267473</td>\n",
       "      <td>1.937736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   num_acc           an       mois       jour            v1  \\\n",
       "count            5314184.0    5314184.0  5314184.0  5314184.0  2.632973e+06   \n",
       "mean   201332794431.786469  2013.327611   6.701324  15.588233 -1.543806e-02   \n",
       "min         200500000001.0       2005.0        1.0        1.0 -1.000000e+00   \n",
       "25%         200800056274.0       2008.0        4.0        8.0  0.000000e+00   \n",
       "50%         201300022821.0       2013.0        7.0       16.0  0.000000e+00   \n",
       "75%         201800041017.0       2018.0       10.0       23.0  0.000000e+00   \n",
       "max         202300054822.0       2023.0       12.0       31.0  9.000000e+00   \n",
       "std       565902720.582649      5.65908   3.374824   8.759571  5.852336e-01   \n",
       "\n",
       "             lartpc        occutc      an_nais        vma  \\\n",
       "count  3.818966e+06  4.076093e+06    5292557.0  1251410.0   \n",
       "mean   6.173533e+00  1.438132e-01  1975.503995  61.816122   \n",
       "min    0.000000e+00  0.000000e+00       1896.0       -1.0   \n",
       "25%    0.000000e+00  0.000000e+00       1963.0       50.0   \n",
       "50%    0.000000e+00  0.000000e+00       1978.0       50.0   \n",
       "75%    0.000000e+00  0.000000e+00       1989.0       80.0   \n",
       "max    9.600000e+02  9.000000e+02       2023.0      901.0   \n",
       "std    2.375976e+01  3.074271e+00     18.55589  26.977133   \n",
       "\n",
       "                            datetime       hour  dayofweek  \n",
       "count                        5314184  5314184.0  5314184.0  \n",
       "mean   2013-11-04 06:06:51.580249344  13.730389   2.984137  \n",
       "min              2005-01-01 00:01:00        0.0        0.0  \n",
       "25%              2008-09-28 17:13:00       10.0        1.0  \n",
       "50%              2013-06-11 22:00:00       15.0        3.0  \n",
       "75%              2018-09-15 16:30:00       18.0        5.0  \n",
       "max              2023-12-31 23:50:00       23.0        6.0  \n",
       "std                              NaN   5.267473   1.937736  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cae964-794e-4a19-a5c3-813fd85920a5",
   "metadata": {},
   "source": [
    "Remove full row duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8df30dc4-bd63-4869-9d98-2a0b33a4ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4644\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.duplicated().sum())\n",
    "merged_df = merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae647e-2127-47bb-943c-e9c65dbfe39f",
   "metadata": {},
   "source": [
    "Check the data balance and calculate the proportion of the most common value per column. Flag as \"Unbalanced\" if that value exceeds 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76488e18-7435-4e89-a311-aa31d1791776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Column  Most Frequent Value %     Balance  Most Frequent Value\n",
      "0            num_acc                   0.09    Balanced                  NaN\n",
      "1                 an                   7.05    Balanced                  NaN\n",
      "2               mois                   9.48    Balanced                  NaN\n",
      "3               jour                   3.44    Balanced                  NaN\n",
      "4               hrmn                   1.26    Balanced                  NaN\n",
      "5                lum                  69.61    Balanced                  NaN\n",
      "6                agg                  61.65    Balanced                  NaN\n",
      "7                int                  68.14    Balanced                  NaN\n",
      "8                atm                  80.52  Unbalanced                  1.0\n",
      "9                col                  32.45    Balanced                  NaN\n",
      "10               com                   2.59    Balanced                  NaN\n",
      "11               adr                  12.95    Balanced                  NaN\n",
      "12               gps                  60.72    Balanced                  NaN\n",
      "13               lat                  37.94    Balanced                  NaN\n",
      "14              long                  37.94    Balanced                  NaN\n",
      "15               dep                   6.73    Balanced                  NaN\n",
      "16              catr                  43.07    Balanced                  NaN\n",
      "17              voie                  31.56    Balanced                  NaN\n",
      "18                v1                  50.45    Balanced                  NaN\n",
      "19                v2                  94.63  Unbalanced                  NaN\n",
      "20              circ                  60.27    Balanced                  NaN\n",
      "21               nbv                  49.16    Balanced                  NaN\n",
      "22                pr                  34.40    Balanced                  NaN\n",
      "23               pr1                  34.54    Balanced                  NaN\n",
      "24              vosp                  92.34  Unbalanced                  0.0\n",
      "25              prof                  77.50  Unbalanced                  1.0\n",
      "26              plan                  79.82  Unbalanced                  1.0\n",
      "27            lartpc                  57.69    Balanced                  NaN\n",
      "28           larrout                  16.38    Balanced                  NaN\n",
      "29              surf                  78.91  Unbalanced                  1.0\n",
      "30             infra                  86.70  Unbalanced                  0.0\n",
      "31              situ                  88.95  Unbalanced                  1.0\n",
      "32              env1                  43.16    Balanced                  NaN\n",
      "33              senc                  65.15    Balanced                  NaN\n",
      "34              catv                  65.73    Balanced                  NaN\n",
      "35            occutc                  75.81  Unbalanced                  0.0\n",
      "36               obs                  89.20  Unbalanced                  0.0\n",
      "37              obsm                  73.11  Unbalanced                  2.0\n",
      "38              choc                  36.49    Balanced                  NaN\n",
      "39              manv                  42.21    Balanced                  NaN\n",
      "40           num_veh                  52.16    Balanced                  NaN\n",
      "41             place                  77.64  Unbalanced                  1.0\n",
      "42              catu                  77.60  Unbalanced                  1.0\n",
      "43              grav                  44.41    Balanced                  NaN\n",
      "44              sexe                  67.42    Balanced                  NaN\n",
      "45            trajet                  35.88    Balanced                  NaN\n",
      "46              secu                  46.21    Balanced                  NaN\n",
      "47              locp                  81.98  Unbalanced                  0.0\n",
      "48              actp                  70.84  Unbalanced                  0.0\n",
      "49             etatp                  70.86  Unbalanced                  0.0\n",
      "50           an_nais                   2.46    Balanced                  NaN\n",
      "51      num_veh_usag                  51.53    Balanced                  NaN\n",
      "52               vma                  76.44  Unbalanced                  NaN\n",
      "53       id_vehicule                  76.44  Unbalanced                  NaN\n",
      "54             motor                  76.44  Unbalanced                  NaN\n",
      "55  id_vehicule_usag                  76.44  Unbalanced                  NaN\n",
      "56             secu1                  76.44  Unbalanced                  NaN\n",
      "57             secu2                  76.44  Unbalanced                  NaN\n",
      "58             secu3                  76.44  Unbalanced                  NaN\n",
      "59         id_usager                  84.95  Unbalanced                  NaN\n",
      "60          datetime                   0.09    Balanced                  NaN\n",
      "61              hour                   9.31    Balanced                  NaN\n",
      "62         dayofweek                  16.74    Balanced                  NaN\n",
      "63      weekday_name                  16.74    Balanced                  NaN\n",
      "64        hrmn_clean                   1.60    Balanced                  NaN\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "balance_all_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Column\": col,\n",
    "        \"Most Frequent Value %\": round((vc := merged_df[col].value_counts(normalize=True, dropna=False)).iloc[0] * 100, 2),\n",
    "        \"Balance\": \"Unbalanced\" if vc.iloc[0] > 0.7 else \"Balanced\",\n",
    "        \"Most Frequent Value\": vc.idxmax() if vc.iloc[0] > 0.7 else None\n",
    "    }\n",
    "    for col in merged_df.columns\n",
    "    if not merged_df[col].value_counts(normalize=True, dropna=False).empty\n",
    "])\n",
    "print(balance_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb824b-b596-4106-a2fd-51af438fac83",
   "metadata": {},
   "source": [
    "Check the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff71d5f8-0a36-4919-891b-805e6da96901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_acc              0.00\n",
      "an                   0.00\n",
      "mois                 0.00\n",
      "jour                 0.00\n",
      "hrmn                 0.00\n",
      "lum                  0.00\n",
      "agg                  0.00\n",
      "int                  0.00\n",
      "atm                  0.00\n",
      "col                  0.00\n",
      "com                  0.00\n",
      "adr                 12.95\n",
      "gps                 60.72\n",
      "lat                 37.94\n",
      "long                37.94\n",
      "dep                  0.00\n",
      "catr                 0.00\n",
      "voie                 8.67\n",
      "v1                  50.45\n",
      "v2                  94.63\n",
      "circ                 0.12\n",
      "nbv                  0.22\n",
      "pr                  34.40\n",
      "pr1                 34.54\n",
      "vosp                 0.22\n",
      "prof                 0.15\n",
      "plan                 0.19\n",
      "lartpc              28.15\n",
      "larrout              9.22\n",
      "surf                 0.15\n",
      "infra                0.46\n",
      "situ                 0.42\n",
      "env1                24.02\n",
      "senc                 0.01\n",
      "catv                 0.00\n",
      "occutc              23.31\n",
      "obs                  0.08\n",
      "obsm                 0.04\n",
      "choc                 0.03\n",
      "manv                 0.05\n",
      "num_veh              0.00\n",
      "place                2.40\n",
      "catu                 0.00\n",
      "grav                 0.00\n",
      "sexe                 0.00\n",
      "trajet               0.02\n",
      "secu                24.69\n",
      "locp                 2.24\n",
      "actp                 2.25\n",
      "etatp                2.25\n",
      "an_nais              0.41\n",
      "num_veh_usag         0.00\n",
      "vma                 76.44\n",
      "id_vehicule         76.44\n",
      "motor               76.44\n",
      "id_vehicule_usag    76.44\n",
      "secu1               76.44\n",
      "secu2               76.44\n",
      "secu3               76.44\n",
      "id_usager           84.95\n",
      "datetime             0.00\n",
      "hour                 0.00\n",
      "dayofweek            0.00\n",
      "weekday_name         0.00\n",
      "hrmn_clean           0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Replace actual NA values and string 'nan' with np.nan\n",
    "merged_df = merged_df.apply(lambda col: col.map(lambda x: np.nan if pd.isna(x) or str(x).strip().lower() == \"nan\" else x))\n",
    "\n",
    "missing_percent = merged_df.isnull().mean().multiply(100).round(2)\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716744a-4e7b-445a-a230-7d58b933704c",
   "metadata": {},
   "source": [
    "Drop some categories with many NaN fields that cannot be used for the further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e2bfba5-a333-46e4-8cff-8cb856b495b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num_acc', 'an', 'mois', 'jour', 'hrmn', 'lum', 'agg', 'int', 'atm', 'col', 'com', 'adr', 'gps', 'lat', 'long', 'dep', 'catr', 'voie', 'circ', 'nbv', 'vosp', 'prof', 'plan', 'larrout', 'surf', 'infra', 'situ', 'senc', 'catv', 'obs', 'obsm', 'choc', 'manv', 'num_veh', 'place', 'catu', 'grav', 'sexe', 'trajet', 'secu', 'locp', 'actp', 'etatp', 'an_nais', 'num_veh_usag', 'secu1', 'secu2', 'secu3', 'datetime', 'hour', 'dayofweek', 'weekday_name', 'hrmn_clean']\n"
     ]
    }
   ],
   "source": [
    "merged_df.drop(columns=[c for c in [\"v1\", \"v2\", \"pr\", \"pr1\", \"lartpc\", \"env1\", \"occutc\", \"vma\",\n",
    "                                    \"id_vehicule\", \"motor\", \"id_vehicule_usag\", \"id_usager\"]\n",
    "                        if c in merged_df.columns], inplace=True)\n",
    "print(merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792f65c-dbbb-4ccc-8458-78e786018d08",
   "metadata": {},
   "source": [
    "Statistical relationships with the target variable grav (injury severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98d30785-957f-4498-8b3b-c42002735e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top numeric correlations with grav:\n",
      "\n",
      "an_nais      0.100071\n",
      "hour         0.013353\n",
      "dayofweek    0.005026\n",
      "mois         0.002354\n",
      "num_acc      0.001466\n",
      "an           0.001466\n",
      "jour         0.000906\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jl\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Categorical features significantly related to grav (p < 0.05):\n",
      "                         F              p\n",
      "grav                   inf   0.000000e+00\n",
      "catu          6.131109e+04   0.000000e+00\n",
      "gps           4.881921e+04   0.000000e+00\n",
      "lat           4.079111e+04   0.000000e+00\n",
      "agg           3.945956e+04   0.000000e+00\n",
      "adr           3.298174e+04   0.000000e+00\n",
      "place         3.033612e+04   0.000000e+00\n",
      "obs           2.487901e+04   0.000000e+00\n",
      "long          2.286869e+04   0.000000e+00\n",
      "sexe          1.877374e+04   0.000000e+00\n",
      "secu          1.659888e+04   0.000000e+00\n",
      "nbv           1.454244e+04   0.000000e+00\n",
      "plan          1.279565e+04   0.000000e+00\n",
      "obsm          1.273760e+04   0.000000e+00\n",
      "larrout       9.843337e+03   0.000000e+00\n",
      "num_veh_usag  9.443238e+03   0.000000e+00\n",
      "voie          9.006817e+03   0.000000e+00\n",
      "dep           8.081652e+03   0.000000e+00\n",
      "secu3         6.717772e+03   0.000000e+00\n",
      "manv          6.533055e+03   0.000000e+00\n",
      "secu1         6.488565e+03   0.000000e+00\n",
      "etatp         5.963694e+03   0.000000e+00\n",
      "secu2         5.830552e+03   0.000000e+00\n",
      "situ          5.240717e+03   0.000000e+00\n",
      "locp          5.089628e+03   0.000000e+00\n",
      "num_veh       5.070626e+03   0.000000e+00\n",
      "actp          5.040914e+03   0.000000e+00\n",
      "catr          4.149026e+03   0.000000e+00\n",
      "trajet        3.991052e+03   0.000000e+00\n",
      "senc          3.559023e+03   0.000000e+00\n",
      "com           3.230813e+03   0.000000e+00\n",
      "int           2.579192e+03   0.000000e+00\n",
      "circ          1.585950e+03   0.000000e+00\n",
      "atm           1.281395e+03   0.000000e+00\n",
      "col           1.170091e+03   0.000000e+00\n",
      "lum           1.164766e+03   0.000000e+00\n",
      "catv          1.123876e+03   0.000000e+00\n",
      "choc          1.063837e+03   0.000000e+00\n",
      "prof          9.512207e+02   0.000000e+00\n",
      "vosp          8.387493e+02   0.000000e+00\n",
      "hrmn_clean    8.138365e+02   0.000000e+00\n",
      "hrmn          7.540720e+02   0.000000e+00\n",
      "surf          5.919070e+02   0.000000e+00\n",
      "infra         3.292937e+02  6.820062e-284\n",
      "weekday_name  1.951080e+02  1.368200e-167\n"
     ]
    }
   ],
   "source": [
    "# 1. Correlation with numeric columns\n",
    "numeric_cols = [col for col in merged_df.select_dtypes(include=[\"number\"]).columns if col != \"grav\"]\n",
    "correlations = merged_df[numeric_cols].corrwith(merged_df[\"grav\"]).dropna()\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top numeric correlations with grav:\\n\")\n",
    "print(correlations)\n",
    "\n",
    "# 2. ANOVA F-test for categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = merged_df.select_dtypes(include=[\"category\", \"object\", \"string\"]).columns\n",
    "anova_scores = {}\n",
    "for col in categorical_cols:\n",
    "    if merged_df[col].nunique() > 1:\n",
    "        try:\n",
    "            X = LabelEncoder().fit_transform(merged_df[col].astype(str))\n",
    "            y = merged_df[\"grav\"]\n",
    "            f_val, p_val = f_classif(X.reshape(-1, 1), y)\n",
    "            anova_scores[col] = (f_val[0], p_val[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "anova_df = pd.DataFrame(anova_scores, index=[\"F\", \"p\"]).T.sort_values(\"F\", ascending=False)\n",
    "anova_df = anova_df[anova_df[\"p\"] < 0.05]\n",
    "\n",
    "print(\"\\n Categorical features significantly related to grav (p < 0.05):\")\n",
    "print(anova_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70fca175-a24b-4d01-be00-2e9cc19b6e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed data saved to: ../data/processed/accidents_processed.csv\n"
     ]
    }
   ],
   "source": [
    "#extract processed csv\n",
    "output_path = \"../data/processed/accidents_processed.csv\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Processed data saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
